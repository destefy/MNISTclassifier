{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be5076d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "from d2l import torch as d2l\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from matplotlib import pyplot as plt\n",
    "# plt.rcParams.update({'figure.max_open_warning': 0})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14c04f5",
   "metadata": {},
   "source": [
    "# Idea\n",
    "\n",
    "This is a simple classification model that aims to classify the classic MNIST hand-drawn number dataset. Given a 28x28 black and white image, we categorize the input into one of 10 digits.\n",
    "\n",
    "The model uses two core principals:\n",
    "- Softmax Regression\n",
    "- Minibatch Stocastic Gradient Descent (MSGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5193055c",
   "metadata": {},
   "source": [
    "We can think of the classification problem as trying to perform regression for each of the 10 possible outputs. We can solve each of these regression problems using (Minibatch Stocastic) Gradient Descent. Our model will then generate the probabilties that any given input corresponds to one of 10 outputs.\n",
    "\n",
    "Because we are dealing with probabilities, using MSGD straight away leads to two problems\n",
    "- The sum of the output probabilties is not guaranteed to be = 1\n",
    "- Probabiltities are not guaranteed to be > 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34333f4f",
   "metadata": {},
   "source": [
    "We use Softmax to fix these problems. \n",
    "Softmax takes an input X and transforms it into a form with positive values and normalized rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479ee272",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X):\n",
    "    X_exp = torch.exp(X)\n",
    "    partition = X_exp.sum(1, keepdims=True)\n",
    "    return X_exp / partition  # The broadcasting mechanism is applied here to fill the tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ef0c7a",
   "metadata": {},
   "source": [
    "Implementation for the softmax classification algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4999147",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySoftmaxRegression(d2l.Classifier):\n",
    "    def __init__(self, num_inputs, num_outputs, lr, sigma=0.01):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.W = torch.normal(0, sigma, size=(num_inputs, num_outputs),\n",
    "                              requires_grad=True)\n",
    "        self.b = torch.zeros(num_outputs, requires_grad=True)\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.W, self.b]\n",
    "\n",
    "    def forward(self, X):\n",
    "        return softmax(torch.matmul(X.reshape((-1, self.W.shape[0])), self.W) + self.b)\n",
    "\n",
    "# the loss function uses cross entropy loss\n",
    "    def loss(self, y_hat, y):\n",
    "        return - torch.log(y_hat[list(range(len(y_hat))), y]).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5917b8",
   "metadata": {},
   "source": [
    "Data to classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95d487a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST(d2l.DataModule):  #@save\n",
    "    def __init__(self, batch_size=64, resize=(28, 28)):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        #defines how to reshape the inputs\n",
    "        trans = transforms.Compose([transforms.Resize(resize), transforms.ToTensor()]) \n",
    "        self.trainingData = torchvision.datasets.MNIST(root=self.root, train=True, \n",
    "                                                       transform=trans, download=True)\n",
    "        self.testingData = torchvision.datasets.MNIST(root=self.root, train=False, \n",
    "                                                      transform=trans, download=True)\n",
    "\n",
    "    def text_labels(self, indices):\n",
    "        labels = ['0','1','2','3','4','5','6','7','8','9']\n",
    "        return [labels[int(i)] for i in indices]\n",
    "\n",
    "    # for each iteration, we read a minibatch of randomly shuffled data \n",
    "    def get_dataloader(self, train):\n",
    "        data = self.trainingData if train else self.testingData\n",
    "        return torch.utils.data.DataLoader(data, self.batch_size, shuffle=train, num_workers=self.num_workers)\n",
    "    \n",
    "    def visualize(self, batch, nrows=1, ncols=8, labels=[]):\n",
    "        X, y = batch\n",
    "        if not labels:\n",
    "            labels = self.text_labels(y)\n",
    "        d2l.show_images(X.squeeze(1), nrows, ncols, titles=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b26a9b",
   "metadata": {},
   "source": [
    "# TRAINING\n",
    "This uses mini-batch stocastic gradient descent with batch_size = 256\n",
    "we treat each 28x28 image as a vector with 784 entries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a056e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = MNIST(batch_size=256)\n",
    "model = MySoftmaxRegression(num_inputs=784, num_outputs=10, lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7827fe1f",
   "metadata": {},
   "source": [
    "The built in fit function grabs a minibatch and passes over the entire dataset to computer the loss.\n",
    "Next the gradient is found w/rt each parameter, and the optimization algorithm updates parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab9214f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = d2l.Trainer(max_epochs=10)\n",
    "trainer.fit(model, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a484d1",
   "metadata": {},
   "source": [
    "# TESTING\n",
    "Use trained model to predict classification for new images. \n",
    "Grab a batch of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7736c4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = next(iter(data.get_dataloader(False)))\n",
    "preds = model(X).argmax(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1adbdbe3",
   "metadata": {},
   "source": [
    "# Find the accuracy of the model after 10 epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c7a999",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "accuracy = MySoftmaxRegression.accuracy(model, model(X), y)\n",
    "print(\"accuracy: \", accuracy.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f52ed6",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "# Show some correctly labeled images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf48689",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = preds.type(y.dtype) == y\n",
    "Xc, yc, predsc = X[correct], y[correct], preds[correct]\n",
    "labels = [\"guessed: \"+ b +'\\n'+ \"answer: \" + a for a, b in zip(data.text_labels(yc), data.text_labels(predsc))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a894a5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.close('all')\n",
    "imgs = Xc.squeeze(1)\n",
    "scale=1.5\n",
    "num_cols=1\n",
    "num_rows=1\n",
    "titles=labels\n",
    "# data.visualize(imgs, ncols=2, labels=labels)\n",
    "\n",
    "figsize = (num_cols * scale, num_rows * scale)\n",
    "_, axes = d2l.plt.subplots(num_rows, num_cols, figsize=figsize)\n",
    "print(axes)\n",
    "if num_cols != 1:\n",
    "    axes = axes.flatten()\n",
    "else:\n",
    "    axes = [axes]\n",
    "print(axes)\n",
    "for i, (ax, img) in enumerate(zip(axes, imgs)):\n",
    "    try:\n",
    "        img = d2l.numpy(img)\n",
    "    except:\n",
    "        pass\n",
    "    ax.imshow(img)\n",
    "    ax.axes.get_xaxis().set_visible(False)\n",
    "    ax.axes.get_yaxis().set_visible(False)\n",
    "    if titles:\n",
    "        ax.set_title(titles[i])\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6f388b",
   "metadata": {},
   "source": [
    "# Show the incorrectly labeled images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b4f8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong = preds.type(y.dtype) != y\n",
    "Xi, yi, predsi = X[wrong], y[wrong], preds[wrong]\n",
    "labels = [\"guessed: \"+ b +'\\n'+ \"correct: \" + a for a, b in zip(data.text_labels(yi), data.text_labels(predsi))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f40d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.close('all')\n",
    "data.visualize([Xi, yi], labels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb4e6be",
   "metadata": {},
   "source": [
    "Most code from the notebook is taken from the D2L textbook:\n",
    "\n",
    "Dive into Deep Learning, Zhang, Aston and Lipton, Zachary C. and Li, Mu and Smola, Alexander J., arXiv preprint arXiv:2106.11342, 2021"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
